<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural language processing on Tanay Kumar Saha</title>
    <link>https://tksaha.github.io/tags/natural-language-processing/</link>
    <description>Recent content in Natural language processing on Tanay Kumar Saha</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Tanay Kumar Saha</copyright>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://tksaha.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Representation Learning of Network Units</title>
      <link>https://tksaha.github.io/project/representation-netunit/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://tksaha.github.io/project/representation-netunit/</guid>
      <description>In classical machine learning, hand-designed features are used for learning a mapping from the features. However, recently, there is a surge of research in representation learning which aims to learn abstract features given the input. For networks, representation learned for nodes and edges has been used for tasks, such as link prediction, collective classification, and many others. In my PhD thesis, I have crafted methods for learning representation for both the sentences and the network nodes/edges.</description>
    </item>
    
  </channel>
</rss>